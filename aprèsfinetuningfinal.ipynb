{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhlNE9lqz6Nh"
   },
   "source": [
    "# <span style=\"color:#2E86C1; font-size:2.5em; font-family:Georgia; font-weight:bold;\">MODÈLE ROBERTA</span>\n",
    "\n",
    "---\n",
    "<div style=\"background-color: #FFB6C1 ; padding: 10px; border-radius: 5px;\">\n",
    "Après avoir testé plusieurs modèles, notamment <strong>GPT-2</strong>, <strong>LLaMA-2</strong>, et <strong>RoBERTa</strong>, notre choix s’est porté sur <strong>RoBERTa</strong>. Ce modèle a démontré une performance supérieure pour la compréhension et l’analyse de texte complexe. Contrairement à <strong>GPT-2</strong>, conçu principalement pour la génération de texte, <strong>RoBERTa</strong> se distingue par sa capacité à saisir les nuances contextuelles et à exceller dans des tâches telles que l'analyse de sentiments et la classification de texte. Bien que <strong>LLaMA-2</strong> soit également performant, <strong>RoBERTa</strong> s'est avéré plus précis et mieux adapté à notre jeu de données. Son efficacité et sa précision en font un choix optimal pour les applications nécessitant une compréhension fine du langage naturel.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpU4OzN13oX9"
   },
   "source": [
    "## Importation des biblios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgNacEkU3otL"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "from rouge_score import rouge_scorer\n",
    "import evaluate\n",
    "import warnings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "677KzvZBCKSk",
    "outputId": "c6736edd-e998-417a-b2d7-a80a7ffda99c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.0+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
      "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.44.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIw8R95T7LPT"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login('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')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVaVAXT-1Pxi"
   },
   "source": [
    "## 1-Zero shot prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZeqwrdBdGdr"
   },
   "source": [
    "<div style=\"background-color: #87CEEB; padding: 10px; border-radius: 5px;\">\n",
    "    En apprentissage zéro-shot, le modèle est amené à accomplir une tâche sans avoir été explicitement formé à cette tâche. Par exemple, en lui posant une question comme \"Quelles sont les métriques clés pour mesurer le succès des entreprises dans l'industrie technologique ?\", on lui demande de générer une réponse sans formation spécifique sur cette tâche. Le modèle doit alors s'appuyer sur ses connaissances préexistantes acquises lors de sa phase de pré-entraînement pour répondre à la question. Cela repose sur sa capacité à généraliser et à utiliser l'information disponible pour accomplir des tâches pour lesquelles il n'a pas reçu d'exemples directs pendant l'entraînement.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNnJDYv4mrBH"
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_path: str):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "def create_qa_pipeline(model, tokenizer):\n",
    "    return pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "def generate_answer(qa_pipeline, question: str, context: str) -> str:\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    return result[0]['answer'] if isinstance(result, list) else result['answer']\n",
    "\n",
    "def evaluate_answer(reference_answer, generated_answer):\n",
    "    if isinstance(reference_answer, list):\n",
    "        reference_answer = ' '.join([str(item) for item in reference_answer])\n",
    "    if isinstance(generated_answer, list):\n",
    "        generated_answer = ' '.join([item['answer'] for item in generated_answer])\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(reference_answer, generated_answer)\n",
    "    return rouge_scores\n",
    "\n",
    "\n",
    "def calculate_execution_time(func, *args, **kwargs):\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return result, execution_time\n",
    "\n",
    "\n",
    "\n",
    "def print_model_params(model):\n",
    "    param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model Parameter Count: {param_count}\")\n",
    "\n",
    "def test_model_with_different_configurations(model_path: str, context: str, question: str, reference_answer: str):\n",
    "    model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "    qa_pipeline = create_qa_pipeline(model, tokenizer)\n",
    "\n",
    "    answer, execution_time = calculate_execution_time(generate_answer, qa_pipeline, question, context)\n",
    "    print(f\"Generated Answer: {answer}\")\n",
    "    print(f\"Execution Time: {execution_time:.4f} seconds\")\n",
    "\n",
    "    rouge_scores = evaluate_answer(reference_answer, answer)\n",
    "    print(f\"ROUGE Scores: {rouge_scores}\")\n",
    "\n",
    "    print_model_params(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0jokyX0ApKtd",
    "outputId": "3fa5e08e-c5c1-4a37-c7b6-16742f11b97d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of France?\n",
      "Generated Answer: Paris\n",
      "Execution Time: 0.0114 seconds\n",
      "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=0.16666666666666666, fmeasure=0.2857142857142857), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=1.0, recall=0.16666666666666666, fmeasure=0.2857142857142857)}\n",
      "--------------------------------------------------\n",
      "Model Parameter Count: 124056578\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_path = \"deepset/roberta-base-squad2\"\n",
    "    question = \"What is the capital of France?\"\n",
    "    context = \"France is a country in Western Europe. The capital of France is Paris.\"\n",
    "    reference_answer = \"The capital of france is Paris\"\n",
    "    test_model_with_different_configurations(model_path, context, question, reference_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7q_0WP4nXSv"
   },
   "source": [
    "<h3 style=\"color: #007acc;\">Résultats pour la question : \"Quelle est la capitale de la France ?\"</h3>\n",
    "\n",
    "<ul>\n",
    "    <li><strong style=\"color: #28a745;\">Réponse générée</strong> : Le modèle a correctement généré <strong>\"Paris\"</strong> comme réponse.</li>\n",
    "    <li><strong style=\"color: #28a745;\">Temps d'exécution</strong> : Le modèle a mis <strong>0,0114 secondes</strong> pour générer la réponse. Ce temps est très rapide, comme on peut s'y attendre pour la plupart des modèles pré-entraînés modernes lorsqu'ils sont exécutés sur un GPU.</li>\n",
    "</ul>\n",
    "\n",
    "<hr style=\"border: 1px solid #ddd;\" />\n",
    "\n",
    "<h4 style=\"color: #007acc;\">Scores ROUGE :</h4>\n",
    "\n",
    "<ul>\n",
    "    <li><strong style=\"color: #ff5733;\">ROUGE-1</strong> :\n",
    "        <ul>\n",
    "            <li><strong>Précision</strong> : <span style=\"color: #28a745;\">1.0</span></li>\n",
    "            <li><strong>Rappel</strong> : <span style=\"color: #dc3545;\">0.167</span></li>\n",
    "            <li><strong>F-mesure</strong> : <span style=\"color: #ffc107;\">0.286</span></li>\n",
    "        </ul>\n",
    "        <p style=\"color: #333333;\">\n",
    "            Ce score indique que la réponse générée par le modèle (<em>\"Paris\"</em>) contient certains mots pertinents de la réponse de référence. Cependant, le rappel est faible, suggérant que la réponse de référence contient plus d'informations pertinentes que la réponse du modèle.\n",
    "        </p>\n",
    "    </li>\n",
    "\n",
    "<li>\n",
    "<strong style=\"color: #ff5733;\">ROUGE-2</strong> :\n",
    "        <ul>\n",
    "            <li><strong>Précision</strong> : <span style=\"color: #dc3545;\">0.0</span></li>\n",
    "            <li><strong>Rappel</strong> : <span style=\"color: #dc3545;\">0.0</span></li>\n",
    "            <li><strong>F-mesure</strong> : <span style=\"color: #dc3545;\">0.0</span></li>\n",
    "        </ul>\n",
    "        <p style=\"color: #333333;\">\n",
    "            Le score ROUGE-2 est nul, ce qui signifie qu'aucun bigramme (combinaison de deux mots) de la réponse générée ne correspond à ceux de la réponse de référence.\n",
    "        </p>\n",
    "    </li>\n",
    "\n",
    "<li><strong style=\"color: #ff5733;\">ROUGE-L</strong> :\n",
    "        <ul>\n",
    "            <li><strong>Précision</strong> : <span style=\"color: #28a745;\">1.0</span></li>\n",
    "            <li><strong>Rappel</strong> : <span style=\"color: #dc3545;\">0.167</span></li>\n",
    "            <li><strong>F-mesure</strong> : <span style=\"color: #ffc107;\">0.286</span></li>\n",
    "        </ul>\n",
    "        <p style=\"color: #333333;\">\n",
    "            Similaire à ROUGE-1, mais prenant en compte les plus longues sous-séquences communes, ce score montre que la séquence de mots communs entre la réponse générée et la réponse de référence est très limitée.\n",
    "        </p>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<hr style=\"border: 1px solid #ddd;\" />\n",
    "\n",
    "<h4 style=\"color: #007acc;\">Paramètres du modèle :</h4>\n",
    "\n",
    "<ul>\n",
    "    <li><strong style=\"color: #28a745;\">Nombre de paramètres</strong> : 124,056,578</li>\n",
    "    <p style=\"color: #333333;\">\n",
    "        Ce nombre de paramètres est typique pour un modèle de transformeur pré-entraîné de grande taille.\n",
    "    </p>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEAIqD-p5EV7"
   },
   "source": [
    "## <strong style=\"color: #007acc;\">Conclusion :</strong>\n",
    "\n",
    "<p style=\"color: #333333;\">\n",
    "    La fonction fonctionne comme prévu, et les résultats donnent un aperçu des performances du modèle pour la tâche de questions-réponses. Les scores ROUGE indiquent que bien que la réponse soit correcte, le chevauchement avec la réponse de référence (s'il s'agissait d'une réponse de plusieurs mots) peut être limité.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qV3JMD1n5NVn"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "    question = \"What is the capital of France?\"\n",
    "    context = \"France is a country in Western Europe. The capital of France is Paris.\"\n",
    "    reference_answer = \"The capital of france is Paris\"\n",
    "    test_model_with_different_configurations(model_path, context, question, reference_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_2mNfoxqSj_"
   },
   "source": [
    "## 2-One shot prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8F_3xlN3qW9V"
   },
   "source": [
    "<div style=\"background-color: #87CEEB; padding: 10px; border-radius: 5px;\">  L'ingénierie de prompt \"One-Shot\" (ou \"prompt à un seul essai\") désigne une approche où un modèle d'IA, comme un modèle de langage, reçoit un seul exemple de tâche ou de question avant d'être utilisé pour générer une réponse. Contrairement à l'approche \"few-shot\" (avec quelques exemples) ou \"zero-shot\" (sans exemple), l'approche \"one-shot\" se base sur l'idée qu'un seul exemple suffira à guider correctement le modèle pour accomplir la tâche demandée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSiiPLOQsyVu"
   },
   "outputs": [],
   "source": [
    "def generate_answer_one_shot(qa_pipeline, question: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an answer based on a one-shot prompt.\n",
    "    This function uses multiple example (question-answer pairs) to teach the model how to answer a question.\n",
    "    \"\"\"\n",
    "    one_shot_prompt = f\"\"\"\n",
    "    Context: {context}\n",
    "\n",
    "    Example 1:\n",
    "    Question: What is the capital of France?\n",
    "    Answer: Paris\n",
    "\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = {\n",
    "        'question': question,\n",
    "        'context': context\n",
    "    }\n",
    "\n",
    "    result = qa_pipeline(inputs)\n",
    "\n",
    "    return result['answer']\n",
    "\n",
    "def evaluate_answer_one_shot(reference_answer: str, generated_answer: str):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(reference_answer, generated_answer)\n",
    "    return rouge_scores\n",
    "\n",
    "def test_model_with_different_configurations(model_path: str, context: str, question: str, reference_answer: str):\n",
    "    model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "    qa_pipeline = create_qa_pipeline(model, tokenizer)\n",
    "    generated_answer, execution_time = calculate_execution_time(generate_answer_one_shot, qa_pipeline, question, context)\n",
    "    rouge_scores = evaluate_answer_one_shot(reference_answer, generated_answer)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Generated Answer: {generated_answer}\")\n",
    "    print(f\"Execution Time: {execution_time:.4f} seconds\")\n",
    "    print(f\"ROUGE Scores: {rouge_scores}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    print_model_params(model)\n",
    "\n",
    "def print_model_params(model):\n",
    "    param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model Parameter Count: {param_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nURG57q9qruo",
    "outputId": "d75b6874-495b-4f80-b1d0-8734f6cd7b15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where is the Eiffel Tower located?\n",
      "Generated Answer: Paris\n",
      "Execution Time: 0.2810 seconds\n",
      "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=0.5, fmeasure=0.6666666666666666), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=1.0, recall=0.5, fmeasure=0.6666666666666666)}\n",
      "--------------------------------------------------\n",
      "Model Parameter Count: 124056578\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_path = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "    context = \"\"\"\n",
    "    Paris is the capital of France. It is one of the most important cultural and economic centers in Europe.\n",
    "    The city is known for its art, fashion, and landmarks like the Eiffel Tower and the Louvre Museum.\n",
    "    \"\"\"\n",
    "\n",
    "    question = \"Where is the Eiffel Tower located?\"\n",
    "    reference_answer = \"Paris, France\"\n",
    "\n",
    "    test_model_with_different_configurations(model_path, context, question, reference_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNQGKLrZvlH0"
   },
   "source": [
    "# Analyse du modèle\n",
    "\n",
    "Le modèle a bien identifié **Paris** comme étant la localisation de la Tour Eiffel, mais il a omis le contexte plus large de **Paris, France** dans la réponse de référence.\n",
    "\n",
    "### Résultats ROUGE :\n",
    "- **Précision** : Le modèle a généré des mots pertinents, mais a manqué des détails importants (ex : le pays **France**).  \n",
    "  <span style=\"color: green;\">**Bonne précision**</span>, mais manque des éléments de contexte.\n",
    "- **Rappel** : Le modèle n'a pas capturé tous les éléments de la réponse complète.  \n",
    "  <span style=\"color: red;\">**Rappel modéré**</span>, le modèle pourrait manquer certaines informations essentielles.\n",
    "- **ROUGE-2 (Bigrammes)** : L'absence de chevauchement de bigrammes suggère une structure de réponse simplifiée et manquant de nuances.  \n",
    "  <span style=\"color: orange;\">**Structure simplifiée**</span>, manque de détails dans les relations entre les mots.\n",
    "- **Exécution rapide** : Le modèle a répondu rapidement, malgré les lacunes de détail.  \n",
    "  <span style=\"color: blue;\">**Exécution rapide**</span>, mais manque de profondeur dans la réponse.\n",
    "\n",
    "### Conclusion :\n",
    "Bien que le modèle ait montré une **bonne précision**, il peut être amélioré en termes de **rappel** et de structure de la réponse pour mieux refléter le contexte complet.  \n",
    "  <span style=\"color: purple;\">**Amélioration possible en termes de rappel et de structure**</span>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRnlVrmavpyf"
   },
   "source": [
    "## 3 -Few shot prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4veKJA-VTOio"
   },
   "source": [
    "<div style=\"background-color: #87CEEB; padding: 10px; border-radius: 5px;\">Le \"few-shot prompt engineering\" est une technique utilisée dans le traitement automatique du langage naturel (NLP) pour orienter les modèles de langage de grande taille (LLMs) comme GPT à effectuer des tâches spécifiques avec un minimum d'exemples. Ici, \"few-shot\" signifie fournir au modèle quelques exemples de paires entrée-sortie dans le prompt afin de démontrer le comportement souhaité avant qu'il ne génère la réponse pour une nouvelle requête."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vs_HT7CIv3J6"
   },
   "outputs": [],
   "source": [
    "def generate_answer_few_shots(qa_pipeline, question: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an answer based on a few-shot prompt.\n",
    "    This function uses multiple example (question-answer pairs) to teach the model how to answer a question.\n",
    "    \"\"\"\n",
    "    few_shot_prompt = f\"\"\"\n",
    "    Context: {context}\n",
    "\n",
    "  Example 1:\n",
    "    Question: What is the capital of France?\n",
    "    Answer: Paris\n",
    "\n",
    "    Example 2:\n",
    "    Question: Where is the Eiffel Tower located?\n",
    "    Answer: Paris\n",
    "\n",
    "    Example 3:\n",
    "    Question: What is Paris known for?\n",
    "    Answer: Art, fashion, and landmarks like the Eiffel Tower and the Louvre Museum.\n",
    "\n",
    "    Example 4:\n",
    "    Question: What museum is in Paris?\n",
    "    Answer: Louvre Museum\n",
    "\n",
    "    Example 5:\n",
    "    Question: What is the population of Paris?\n",
    "    Answer: Paris has a population of around 2.1 million people within the city limits. The metropolitan area has a population of over 12 million.\n",
    "\n",
    "    Example 6:\n",
    "    Question: How old is the Eiffel Tower?\n",
    "    Answer: The Eiffel Tower was completed in 1889, making it over 130 years old.\n",
    "\n",
    "    Example 7:\n",
    "    Question: What is the famous landmark in Paris known for its glass pyramid?\n",
    "    Answer: The Louvre Museum, which has a famous glass pyramid entrance.\n",
    "\n",
    "    Example 8:\n",
    "    Question: What type of cuisine is Paris known for?\n",
    "    Answer: Paris is famous for French cuisine, which includes dishes like croissants, escargot, and coq au vin.\n",
    "\n",
    "    Example 9:\n",
    "    Question: Who designed the Eiffel Tower?\n",
    "    Answer: The Eiffel Tower was designed by Gustave Eiffel, a French civil engineer.\n",
    "\n",
    "    Example 10:\n",
    "    Question: Is Paris a coastal city?\n",
    "    Answer: No, Paris is located inland along the River Seine and is not a coastal city.\n",
    "\n",
    "    Example 11:\n",
    "    Question: What is the population of Paris?\n",
    "    Answer: Paris has a population of about 2.1 million people, and the metropolitan area has a population of over 12 million people.\n",
    "\n",
    "    Example 12:\n",
    "    Question: What is the Eiffel Tower known for?\n",
    "    Answer: The Eiffel Tower is famous for being a world-renowned symbol of France, offering stunning views of Paris and being a prime tourist attraction.\n",
    "\n",
    "    Example 13:\n",
    "    Question: What is the Louvre Museum known for?\n",
    "    Answer: The Louvre Museum is famous for housing thousands of works of art, including the Mona Lisa, one of the most famous paintings in the world.\n",
    "\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "    return result[0]['answer'] if isinstance(result, list) else result['answer']\n",
    "\n",
    "def test_few_shot(model_path: str, context: str, questions: list, reference_answers: list):\n",
    "    model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "    qa_pipeline = create_qa_pipeline(model, tokenizer)\n",
    "\n",
    "    for question, reference_answer in zip(questions, reference_answers):\n",
    "        generated_answer, execution_time = calculate_execution_time(generate_answer_few_shots, qa_pipeline, question, context)\n",
    "\n",
    "        rouge_scores = evaluate_answer_one_shot(reference_answer, generated_answer)\n",
    "\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Generated Answer: {generated_answer}\")\n",
    "        print(f\"Execution Time: {execution_time:.4f} seconds\")\n",
    "        print(f\"ROUGE Scores: {rouge_scores}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    print_model_params(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CEBKuWQBwqoV",
    "outputId": "8e8f9bb9-c19c-44ef-ace6-acb6280f61ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of France?\n",
      "Generated Answer: Paris\n",
      "Execution Time: 0.5304 seconds\n",
      "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "--------------------------------------------------\n",
      "Question: Where is the Eiffel Tower located?\n",
      "Generated Answer: Paris\n",
      "Execution Time: 0.5229 seconds\n",
      "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "--------------------------------------------------\n",
      "Question: What is Paris known for?\n",
      "Generated Answer: culinary culture\n",
      "Execution Time: 0.4984 seconds\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n",
      "--------------------------------------------------\n",
      "Question: What museum is in Paris?\n",
      "Generated Answer: Louvre Museum\n",
      "Execution Time: 0.5194 seconds\n",
      "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "--------------------------------------------------\n",
      "Question: What is the population of Paris?\n",
      "Generated Answer: over 2 million\n",
      "Execution Time: 0.5359 seconds\n",
      "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=0.125, fmeasure=0.2222222222222222), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.6666666666666666, recall=0.08333333333333333, fmeasure=0.14814814814814814)}\n",
      "--------------------------------------------------\n",
      "Question: How old is the Eiffel Tower?\n",
      "Generated Answer: 1889\n",
      "Execution Time: 0.5290 seconds\n",
      "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=0.07692307692307693, fmeasure=0.14285714285714288), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=1.0, recall=0.07692307692307693, fmeasure=0.14285714285714288)}\n",
      "--------------------------------------------------\n",
      "Question: What is the famous landmark in Paris known for its glass pyramid?\n",
      "Generated Answer: Eiffel Tower\n",
      "Execution Time: 0.5265 seconds\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n",
      "--------------------------------------------------\n",
      "Question: What type of cuisine is Paris known for?\n",
      "Generated Answer: French\n",
      "Execution Time: 0.8557 seconds\n",
      "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=0.0625, fmeasure=0.11764705882352941), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=1.0, recall=0.0625, fmeasure=0.11764705882352941)}\n",
      "--------------------------------------------------\n",
      "Question: Who designed the Eiffel Tower?\n",
      "Generated Answer: Gustave Eiffel\n",
      "Execution Time: 0.8603 seconds\n",
      "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=0.16666666666666666, fmeasure=0.2857142857142857), 'rouge2': Score(precision=1.0, recall=0.09090909090909091, fmeasure=0.16666666666666669), 'rougeL': Score(precision=1.0, recall=0.16666666666666666, fmeasure=0.2857142857142857)}\n",
      "--------------------------------------------------\n",
      "Question: Is Paris a coastal city?\n",
      "Generated Answer: River Seine\n",
      "Execution Time: 0.8321 seconds\n",
      "ROUGE Scores: {'rouge1': Score(precision=1.0, recall=0.13333333333333333, fmeasure=0.23529411764705882), 'rouge2': Score(precision=1.0, recall=0.07142857142857142, fmeasure=0.13333333333333333), 'rougeL': Score(precision=1.0, recall=0.13333333333333333, fmeasure=0.23529411764705882)}\n",
      "--------------------------------------------------\n",
      "Model Parameter Count: 124056578\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_path = \"deepset/roberta-base-squad2\"\n",
    "    context = \"\"\"\n",
    "    Paris is the capital of France, located on the River Seine. It is famous for its landmarks like the Eiffel Tower, the Louvre Museum, and the Notre-Dame Cathedral. Paris has a population of over 2 million people in the city and over 12 million in the metropolitan area.\n",
    "    The Eiffel Tower was designed by Gustave Eiffel and completed in 1889. It is one of the most iconic landmarks in the world.\n",
    "    Paris is known for its culinary culture, offering French dishes such as croissants, escargot, and coq au vin.\n",
    "    The Louvre Museum, which houses thousands of works of art, including the Mona Lisa, is located in Paris.\n",
    "    \"\"\"\n",
    "\n",
    "    questions = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Where is the Eiffel Tower located?\",\n",
    "        \"What is Paris known for?\",\n",
    "        \"What museum is in Paris?\",\n",
    "        \"What is the population of Paris?\",\n",
    "        \"How old is the Eiffel Tower?\",\n",
    "        \"What is the famous landmark in Paris known for its glass pyramid?\",\n",
    "        \"What type of cuisine is Paris known for?\",\n",
    "        \"Who designed the Eiffel Tower?\",\n",
    "        \"Is Paris a coastal city?\"\n",
    "    ]\n",
    "\n",
    "    reference_answers = [\n",
    "        \"Paris\",\n",
    "        \"Paris\",\n",
    "        \"Art, fashion, and landmarks like the Eiffel Tower and the Louvre Museum.\",\n",
    "        \"Louvre Museum\",\n",
    "        \"Paris has a population of around 2.1 million people within the city limits. The metropolitan area has a population of over 12 million.\",\n",
    "        \"The Eiffel Tower was completed in 1889, making it over 130 years old.\",\n",
    "        \"The Louvre Museum, which has a famous glass pyramid entrance.\",\n",
    "        \"Paris is famous for French cuisine, which includes dishes like croissants, escargot, and coq au vin.\",\n",
    "        \"The Eiffel Tower was designed by Gustave Eiffel, a French civil engineer.\",\n",
    "        \"No, Paris is located inland along the River Seine and is not a coastal city.\"\n",
    "    ]\n",
    "\n",
    "    test_few_shot(model_path, context, questions, reference_answers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmvLQUkaTOip"
   },
   "source": [
    "<h3 style=\"color: #007acc;\">Interprétation</h3>\n",
    "\n",
    "<p style=\"color: #333333;\">\n",
    "    Le modèle fonctionne bien pour les questions <span style=\"color: #28a745; font-weight: bold;\">factuelles, avec réponses directes</span> (par exemple, sur les capitales, les musées, les designers), en obtenant des <span style=\"color: #28a745;\">scores ROUGE élevés</span>.\n",
    "</p>\n",
    "\n",
    "<p style=\"color: #333333;\">\n",
    "    En revanche, pour les questions plus complexes ou dépendant du contexte, comme celles portant sur un <span style=\"color: #ff5733; font-style: italic;\">“célèbre monument avec une pyramide en verre”</span> ou <span style=\"color: #ff5733; font-style: italic;\">“connu pour”</span>, le modèle n'arrive pas à fournir des réponses suffisamment pertinentes, ce qui entraîne des <span style=\"color: #dc3545;\">scores ROUGE plus faibles</span>.\n",
    "</p>\n",
    "\n",
    "<p style=\"color: #333333;\">\n",
    "    Il serait donc bénéfique d'améliorer la capacité du modèle à comprendre les <span style=\"color: #007acc; font-weight: bold;\">questions nuancées</span> ou plus détaillées pour une meilleure performance sur tous types de requêtes.\n",
    "</p>\n",
    "\n",
    "<p style=\"color: #333333;\">\n",
    "    En résumé, bien que le modèle performe bien sur certains types de requêtes, des améliorations sont possibles pour mieux gérer les <span style=\"color: #007acc;\">questions complexes ou contextuelles</span>, en particulier pour renforcer le rappel et la précision dans les scores ROUGE-2 et ROUGE-L.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14PTwV_W1z7e"
   },
   "source": [
    "## PEFT Parameter-Efficient Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2yKwd27TOip"
   },
   "source": [
    "<div style=\"background-color: #87CEEB; padding: 10px; border-radius: 5px;\">\n",
    "Le Parameter-Efficient Fine-Tuning (PEFT) est une technique d'apprentissage automatique qui permet aux modèles de s'adapter à de nouvelles tâches en effectuant des mises à jour minimales de leurs paramètres. Plutôt que de fine-tuner l'ensemble des paramètres du modèle — ce qui peut être coûteux en termes de calcul et de mémoire — PEFT se concentre sur l'ajustement sélectif d'un petit sous-ensemble de paramètres. Cela permet d'apprendre efficacement tout en conservant les connaissances préexistantes du modèle.\n",
    "\n",
    "Cette approche est particulièrement utile dans des contextes où l'adaptation rapide à de nouvelles tâches est requise, sans avoir à réentraîner le modèle entier, ce qui réduit considérablement les ressources nécessaires.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "749453296f1846ba858e063d67655b69",
      "2a0c49020dec4680bcf6205c4ffc6294",
      "d80fd6a5efea44e5a6e9c5ff10e544a9",
      "a19df65cf482477ab95aa8201e41047c",
      "6b2799e24d9a4e2e835a13693d212cc6",
      "5c957206c55740dc8d4f987cf75d04a6",
      "0c58a284f0564ad5bc9b8765c80d383f",
      "62560eee08ba4afab1375d0146dad46b",
      "049f6e54c97f4a7bb6269f51ca3663fa",
      "1d351cf4da9442aeaeb3aa3e297128b7",
      "dd34c17ea0f14a3c8f81bccaf3baa22a",
      "5e77643866d1468a81f6bf8efaa25ed1",
      "880c698bc4a64e5cb177e83836049f5c",
      "310b257905154887b4cb49363923b52b",
      "ce71d8ea497140daab88316c88704061",
      "1bd9a5486b2e48189d6fe208967b1abb",
      "cda7c97514b144cbb3a78ee9c82de964",
      "3bff57bcdff1434a952716c0e4a19894",
      "aadd10fe0aa846cda261db0baae797e4"
     ]
    },
    "id": "mtS4uiq9JoiM",
    "outputId": "ba36f91b-69c6-4b50-a3de-db04cab202ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:kpvzvllm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749453296f1846ba858e063d67655b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.023 MB of 0.023 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>14158947225600.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>9</td></tr><tr><td>train_loss</td><td>2.47605</td></tr><tr><td>train_runtime</td><td>4.6</td></tr><tr><td>train_samples_per_second</td><td>11.739</td></tr><tr><td>train_steps_per_second</td><td>1.957</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">major-morning-50</strong> at: <a href='https://wandb.ai/amira-khalfi-esprit/huggingface/runs/kpvzvllm' target=\"_blank\">https://wandb.ai/amira-khalfi-esprit/huggingface/runs/kpvzvllm</a><br/> View project at: <a href='https://wandb.ai/amira-khalfi-esprit/huggingface' target=\"_blank\">https://wandb.ai/amira-khalfi-esprit/huggingface</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241114_115618-kpvzvllm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:kpvzvllm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241114_115925-sg1q0blg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amira-khalfi-esprit/huggingface/runs/sg1q0blg' target=\"_blank\">desert-plasma-51</a></strong> to <a href='https://wandb.ai/amira-khalfi-esprit/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amira-khalfi-esprit/huggingface' target=\"_blank\">https://wandb.ai/amira-khalfi-esprit/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amira-khalfi-esprit/huggingface/runs/sg1q0blg' target=\"_blank\">https://wandb.ai/amira-khalfi-esprit/huggingface/runs/sg1q0blg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset: Index(['question', 'answer', 'context', 'answer_start'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049f6e54c97f4a7bb6269f51ca3663fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preprocessing time: 0.48 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [339/339 04:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.712300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.713600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.459900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.369800</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.405000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.301400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.641000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.506700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.097700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.734700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.150700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.028800</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 296.36 seconds\n",
      "Total execution time: 299.24 seconds\n",
      "Total parameters: 124351490\n",
      "Trainable parameters: 294912\n"
     ]
    }
   ],
   "source": [
    "qa_model_id = 'deepset/roberta-base-squad2'\n",
    "corpus_file_path = './finance_qa_dataset.csv'\n",
    "\n",
    "wandb.init(project=\"huggingface\", entity=\"amira-khalfi-esprit\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(qa_model_id)\n",
    "model = RobertaForQuestionAnswering.from_pretrained(qa_model_id)\n",
    "\n",
    "def apply_lora(model):\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"QUESTION_ANSWERING\"\n",
    "    )\n",
    "    return get_peft_model(model, lora_config)\n",
    "\n",
    "model = apply_lora(model)\n",
    "\n",
    "def load_dataset_from_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Columns in dataset:\", data.columns)\n",
    "\n",
    "    data['context'] = data['question'].map(context_data)\n",
    "\n",
    "    dataset = Dataset.from_pandas(data)\n",
    "    return dataset\n",
    "\n",
    "def tokenize_data_with_positions(dataset):\n",
    "    def tokenize_function(examples):\n",
    "        answers = examples['answer']\n",
    "        answer_starts = examples['answer_start']\n",
    "\n",
    "        encoding = tokenizer(\n",
    "            examples['question'],\n",
    "            examples['context'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "\n",
    "        for i, answer in enumerate(answers):\n",
    "            start_char = answer_starts[i]\n",
    "            end_char = start_char + len(answer)\n",
    "\n",
    "            if start_char < 0 or end_char > len(examples['context'][i]):\n",
    "                start_position = end_position = 0\n",
    "            else:\n",
    "                start_position = encoding.char_to_token(start_char)\n",
    "                end_position = encoding.char_to_token(end_char - 1)\n",
    "\n",
    "                if start_position is None or end_position is None:\n",
    "                    start_position = end_position = 0\n",
    "\n",
    "            start_positions.append(start_position)\n",
    "            end_positions.append(end_position)\n",
    "\n",
    "        encoding['start_positions'] = start_positions\n",
    "        encoding['end_positions'] = end_positions\n",
    "\n",
    "        return encoding\n",
    "\n",
    "    return dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "def split_dataset(dataset):\n",
    "    data_pandas = dataset.to_pandas()\n",
    "    train_data, eval_data = train_test_split(data_pandas, test_size=0.1, random_state=42)\n",
    "    train_dataset = Dataset.from_pandas(train_data)\n",
    "    eval_dataset = Dataset.from_pandas(eval_data)\n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "start_time = time.time()\n",
    "dataset = load_dataset_from_csv(corpus_file_path)\n",
    "dataset = tokenize_data_with_positions(dataset)\n",
    "train_dataset, eval_dataset = split_dataset(dataset)\n",
    "print(f\"Dataset preprocessing time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"finance-qa-finetuning\"\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    ")\n",
    "\n",
    "start_train_time = time.time()\n",
    "trainer.train()\n",
    "print(f\"Training time: {time.time() - start_train_time:.2f} seconds\")\n",
    "\n",
    "model.save_pretrained('./finance_finetuned_model')\n",
    "tokenizer.save_pretrained('./finance_finetuned_model')\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def display_model_info(model):\n",
    "    \"\"\"\n",
    "    Display the number of parameters in the model.\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "\n",
    "def evaluate_model(eval_dataset, model, tokenizer):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for idx, example in enumerate(eval_dataset):\n",
    "        inputs = tokenizer(example['question'], example['context'], return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        start_idx = torch.argmax(outputs.start_logits)\n",
    "        end_idx = torch.argmax(outputs.end_logits)\n",
    "\n",
    "        answer = tokenizer.decode(inputs['input_ids'][0][start_idx:end_idx + 1], skip_special_tokens=True)\n",
    "\n",
    "        predictions.append({\n",
    "            'id': str(idx),\n",
    "            'prediction_text': answer\n",
    "        })\n",
    "\n",
    "        references.append({\n",
    "            'id': str(idx),\n",
    "            'answers': [{'text': example['answer'], 'answer_start': example['answer_start']}]\n",
    "        })\n",
    "\n",
    "    rouge_results = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "    return  rouge_results\n",
    "\n",
    "\n",
    "print(f\"Total execution time: {time.time() - start_time:.2f} seconds\")\n",
    "display_model_info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lWFzCxXKe1Je",
    "outputId": "2431a212-3cb3-43bd-8a84-ede4feb25db4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModel' is not supported for question-answering. Supported models are ['AlbertForQuestionAnswering', 'BartForQuestionAnswering', 'BertForQuestionAnswering', 'BigBirdForQuestionAnswering', 'BigBirdPegasusForQuestionAnswering', 'BloomForQuestionAnswering', 'CamembertForQuestionAnswering', 'CanineForQuestionAnswering', 'ConvBertForQuestionAnswering', 'Data2VecTextForQuestionAnswering', 'DebertaForQuestionAnswering', 'DebertaV2ForQuestionAnswering', 'DistilBertForQuestionAnswering', 'ElectraForQuestionAnswering', 'ErnieForQuestionAnswering', 'ErnieMForQuestionAnswering', 'FalconForQuestionAnswering', 'FlaubertForQuestionAnsweringSimple', 'FNetForQuestionAnswering', 'FunnelForQuestionAnswering', 'GPT2ForQuestionAnswering', 'GPTNeoForQuestionAnswering', 'GPTNeoXForQuestionAnswering', 'GPTJForQuestionAnswering', 'IBertForQuestionAnswering', 'LayoutLMv2ForQuestionAnswering', 'LayoutLMv3ForQuestionAnswering', 'LEDForQuestionAnswering', 'LiltForQuestionAnswering', 'LlamaForQuestionAnswering', 'LongformerForQuestionAnswering', 'LukeForQuestionAnswering', 'LxmertForQuestionAnswering', 'MarkupLMForQuestionAnswering', 'MBartForQuestionAnswering', 'MegaForQuestionAnswering', 'MegatronBertForQuestionAnswering', 'MistralForQuestionAnswering', 'MixtralForQuestionAnswering', 'MobileBertForQuestionAnswering', 'MPNetForQuestionAnswering', 'MptForQuestionAnswering', 'MraForQuestionAnswering', 'MT5ForQuestionAnswering', 'MvpForQuestionAnswering', 'NemotronForQuestionAnswering', 'NezhaForQuestionAnswering', 'NystromformerForQuestionAnswering', 'OPTForQuestionAnswering', 'QDQBertForQuestionAnswering', 'Qwen2ForQuestionAnswering', 'Qwen2MoeForQuestionAnswering', 'ReformerForQuestionAnswering', 'RemBertForQuestionAnswering', 'RobertaForQuestionAnswering', 'RobertaPreLayerNormForQuestionAnswering', 'RoCBertForQuestionAnswering', 'RoFormerForQuestionAnswering', 'SplinterForQuestionAnswering', 'SqueezeBertForQuestionAnswering', 'T5ForQuestionAnswering', 'UMT5ForQuestionAnswering', 'XLMForQuestionAnsweringSimple', 'XLMRobertaForQuestionAnswering', 'XLMRobertaXLForQuestionAnswering', 'XLNetForQuestionAnsweringSimple', 'XmodForQuestionAnswering', 'YosoForQuestionAnswering'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE score: {'rouge1': 0.888888888888889, 'rouge2': 0.8750000000000001, 'rougeL': 0.888888888888889, 'rougeLsum': 0.888888888888889}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    model, tokenizer = prepare_model_and_tokenizer(qa_model_id)\n",
    "    if model is None or tokenizer is None:\n",
    "        return\n",
    "    model = apply_lora(model)\n",
    "    save_model_and_tokenizer(model, tokenizer)\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "    evaluate_model(qa_pipeline, eval_data)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vo6l09pHTOiq"
   },
   "source": [
    "# Évaluation du Score ROUGE\n",
    "\n",
    "Le score ROUGE (Recall-Oriented Understudy for Gisting Evaluation) est une métrique populaire pour évaluer la génération de texte, la synthèse et la traduction automatique. Voici la répartition du score ROUGE pour le texte généré par rapport à la référence :\n",
    "\n",
    "## <span style=\"color: #1E90FF\">ROUGE-1</span>\n",
    "- **Score** : <span style=\"color: #32CD32\">0.89</span>\n",
    "- **Description** : Mesure le chevauchement des **unigrammes** (mots simples) entre le texte généré et la référence. Un score de 0.89 suggère un **très fort chevauchement** au niveau des unigrammes.\n",
    "\n",
    "## <span style=\"color: #1E90FF\">ROUGE-2</span>\n",
    "- **Score** : <span style=\"color: #32CD32\">0.88</span>\n",
    "- **Description** : Mesure le chevauchement des **bigrams** (paires de mots). Ce score indique une **bonne similarité** au niveau des bigrams.\n",
    "\n",
    "## <span style=\"color: #1E90FF\">ROUGE-L</span>\n",
    "- **Score** : <span style=\"color: #32CD32\">0.89</span>\n",
    "- **Description** : Évalue la **plus longue sous-séquence commune (LCS)**, qui prend en compte l'ordre des mots. Ce score suggère une **forte correspondance séquentielle** entre les textes générés et de référence.\n",
    "\n",
    "## <span style=\"color: #1E90FF\">ROUGE-Lsum</span>\n",
    "- **Score** : <span style=\"color: #32CD32\">0.89</span>\n",
    "- **Description** : Ce score est utilisé pour les **tâches de résumé**, reflétant l'alignement au niveau du résumé. Un score de 0.89 indique un **bon alignement** du résumé généré avec le résumé de référence.\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color: #FF4500\">Résumé</span>\n",
    "- Les scores ROUGE indiquent un **très haut niveau de similarité** entre le contenu généré et la référence, avec une excellente correspondance à la fois en **chevauchement de contenu** (unigrammes et bigrams) et en **structure séquentielle** (LCS).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMeOwVU0X8-K"
   },
   "source": [
    "# Application RAG avec LANGCHAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN8FjkUbpTGh"
   },
   "source": [
    "## Système de Question-Réponse avec LangChain, Hugging Face et FAISS\n",
    "\n",
    "Dans notre travail, nous avons développé un système de **question-réponse** en utilisant **LangChain**, **Hugging Face** et **FAISS** pour la recherche documentaire et la génération de réponses.\n",
    "\n",
    "## Processus clé :\n",
    "\n",
    "### <span style=\"color: green;\">Chargement du modèle QA :</span>\n",
    "Nous avons intégré un modèle pré-entraîné de type **RoBERTa** avec **Hugging Face**, ainsi qu'un tokenizer pour traiter les questions et contextes.\n",
    "\n",
    "### <span style=\"color: blue;\">Création du pipeline QA :</span>\n",
    "Un pipeline permet de générer des réponses à partir d'un contexte donné en utilisant le modèle et le tokenizer.\n",
    "\n",
    "### <span style=\"color: orange;\">Indexation avec FAISS :</span>\n",
    "Les documents sont indexés sous forme d'embeddings générés par **Sentence-Transformers** et **FAISS** est utilisé pour rechercher les documents pertinents.\n",
    "\n",
    "### <span style=\"color: purple;\">Récupération des documents :</span>\n",
    "Lorsqu'une question est posée, **FAISS** recherche les documents les plus pertinents en fonction de l'embedding de la question.\n",
    "\n",
    "### <span style=\"color: red;\">Génération de la réponse :</span>\n",
    "Les documents pertinents sont combinés et utilisés pour générer une réponse via le modèle **RoBERTa**.\n",
    "\n",
    "### <span style=\"color: brown;\">Évaluation avec ROUGE :</span>\n",
    "La qualité des réponses générées est mesurée à l'aide de la métrique **ROUGE**, en comparant les réponses générées à celles de référence.\n",
    "\n",
    "---\n",
    "\n",
    "Ce système combine la **recherche documentaire** et la **génération de texte**, offrant des réponses précises et pertinentes aux questions posées. Le **RAG** (Retrieval-Augmented Generation) est utilisé pour améliorer la génération en augmentant les réponses avec des informations extraites des documents récupérés.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0VsdGkQjwtN",
    "outputId": "c6198928-5ae2-437a-911c-c465ec417e6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-gpu\n",
      "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-gpu\n",
      "Successfully installed faiss-gpu-1.7.2\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cn0rwBxhlB3o",
    "outputId": "ad68a4c6-95c4-4a54-fcc2-10f1c78cedf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.7-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
      "Collecting SQLAlchemy<2.0.36,>=1.4 (from langchain-community)\n",
      "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.10)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.7)\n",
      "Collecting langchain-core<0.4.0,>=0.3.17 (from langchain-community)\n",
      "  Downloading langchain_core-0.3.18-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.142)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.7->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.7->langchain-community) (2.9.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2.0.36,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.17->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.7->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.7->langchain-community) (2.23.4)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n",
      "Downloading langchain_community-0.3.7-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain_core-0.3.18-py3-none-any.whl (409 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
      "Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: SQLAlchemy, python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain-community\n",
      "  Attempting uninstall: SQLAlchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.36\n",
      "    Uninstalling SQLAlchemy-2.0.36:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.36\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.15\n",
      "    Uninstalling langchain-core-0.3.15:\n",
      "      Successfully uninstalled langchain-core-0.3.15\n",
      "Successfully installed SQLAlchemy-2.0.35 dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.7 langchain-core-0.3.18 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.6.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AtTBgLXo0Kt"
   },
   "source": [
    "\n",
    "# <span style=\"color: #1E90FF\"><strong>Le RAG (Retrieval-Augmented Generation)</strong></span>\n",
    "\n",
    "Le RAG est un modèle qui combine la recherche de documents avec la génération de texte. Dans notre approche, le RAG est implémenté à travers l'utilisation de **FAISS** pour la récupération des documents et d'un modèle pré-entraîné de génération de texte (comme RoBERTa) pour générer la réponse. Voici où il s'intègre dans les étapes que nous avons définies :\n",
    "\n",
    "## <span style=\"color: #32CD32\"><strong>Récupération des documents :</strong></span>\n",
    "\n",
    "Lorsqu'une question est posée, un embedding de la question est généré, puis **FAISS** est utilisé pour rechercher dans une base de données d'index des documents pertinents (c'est la phase de récupération).\n",
    "\n",
    "## <span style=\"color: #FF6347\"><strong>Génération de la réponse (augmentation par récupération) :</strong></span>\n",
    "\n",
    "Une fois les documents pertinents récupérés, ils sont combinés pour fournir un contexte qui est passé au modèle génératif (comme RoBERTa) pour générer la réponse.  \n",
    "Ici, le modèle génératif utilise les documents récupérés pour compléter sa réponse, augmentant ainsi la qualité et la pertinence des réponses générées. Cela constitue le **Retrieval-Augmented Generation**.\n",
    "\n",
    "Ainsi, le RAG est intégré dans le pipeline entre la récupération des documents et la génération de la réponse, en augmentant la génération avec des informations pertinentes provenant de documents externes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sWbKcmj3jtOc",
    "outputId": "8dff7e8f-4d12-4f9a-c178-4ab06bc5fe70"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: a place where buyers and sellers come together to trade shares of public companies\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.7857142857142857, recall=0.7333333333333333, fmeasure=0.7586206896551724), 'rouge2': Score(precision=0.6923076923076923, recall=0.6428571428571429, fmeasure=0.6666666666666666), 'rougeL': Score(precision=0.7857142857142857, recall=0.7333333333333333, fmeasure=0.7586206896551724)}\n"
     ]
    }
   ],
   "source": [
    "def load_qa_model(model_name='deepset/roberta-base-squad2'):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def create_qa_pipeline(model, tokenizer):\n",
    "    return pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "\n",
    "def generate_answer(qa_pipeline, question, context):\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    return result['answer']\n",
    "\n",
    "def load_faiss_index(embedding_model, corpus=None):\n",
    "    if corpus is not None:\n",
    "        embeddings = embedding_model.embed_documents(corpus)\n",
    "\n",
    "        embeddings_array = np.array([np.array(embedding) for embedding in embeddings], dtype=np.float32)\n",
    "\n",
    "        index = faiss.IndexFlatL2(embeddings_array.shape[1])\n",
    "        index.add(embeddings_array)\n",
    "\n",
    "        index.documents = corpus\n",
    "    else:\n",
    "        index = faiss.IndexFlatL2(768)\n",
    "        index.documents = []\n",
    "    return index\n",
    "\n",
    "def update_faiss_index(index, new_documents, embedding_model):\n",
    "    new_embeddings = embedding_model.embed_documents(new_documents)\n",
    "\n",
    "    new_embeddings_array = np.array([np.array(embedding) for embedding in new_embeddings], dtype=np.float32)\n",
    "\n",
    "    index.add(new_embeddings_array)\n",
    "\n",
    "    index.documents.extend(new_documents)\n",
    "\n",
    "def retrieve_documents(query, index, embedding_model, k=5):\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "\n",
    "    distances, indices = index.search(np.array([query_embedding]).astype(np.float32), k)\n",
    "\n",
    "    return [index.documents[i] for i in indices[0]]\n",
    "\n",
    "def test_rag_system(query, reference_answer):\n",
    "    model, tokenizer = load_qa_model()\n",
    "    qa_pipeline = create_qa_pipeline(model, tokenizer)\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    corpus = [\n",
    "      \"The stock market is a place where buyers and sellers come together to trade shares of public companies.\",\n",
    "    \"The Federal Reserve controls monetary policy and can influence interest rates and inflation.\",\n",
    "    \"A mutual fund is a pool of funds collected from many investors for the purpose of investing in securities.\",\n",
    "    \"Cryptocurrency is a form of digital or virtual currency that relies on cryptographic methods for security.\",\n",
    "    \"Financial planning involves setting goals, assessing your financial situation, and creating a strategy to achieve your goals.\",\n",
    "    \"A hedge fund is an investment vehicle that pools capital from accredited individuals or institutional investors to invest in a variety of assets.\",\n",
    "    \"An index fund is a type of mutual fund designed to replicate the performance of a specific index.\",\n",
    "    \"Bonds are fixed-income securities where investors lend money to an entity for a fixed period of time in exchange for interest payments.\",\n",
    "    \"Stocks represent ownership in a company and entitle the shareholder to a portion of the company's profits.\",\n",
    "    \"The capital market is a market for buying and selling financial securities, like stocks and bonds.\",\n",
    "    \"The bond market refers to the marketplace where participants can issue new debt or buy and sell debt securities.\",\n",
    "    \"A certificate of deposit (CD) is a time deposit offered by banks with a fixed interest rate and maturity date.\",\n",
    "    \"A savings account is a deposit account held at a financial institution that provides a modest interest rate.\",\n",
    "    \"The Dow Jones Industrial Average is a stock market index that tracks 30 large publicly-owned companies in the United States.\",\n",
    "    \"The S&P 500 is a stock market index that tracks 500 large companies listed on stock exchanges in the United States.\",\n",
    "    \"An exchange-traded fund (ETF) is a type of fund that holds assets like stocks, commodities, or bonds and is traded on a stock exchange.\",\n",
    "    \"A credit score is a numerical expression based on a person's credit history, used by lenders to assess creditworthiness.\",\n",
    "    \"Interest rates represent the cost of borrowing money, typically expressed as an annual percentage rate (APR).\",\n",
    "    \"An IPO (Initial Public Offering) is the process through which a private company offers shares to the public for the first time.\",\n",
    "    \"A 401(k) is a retirement savings plan sponsored by an employer that allows employees to save and invest for retirement on a tax-deferred basis.\",\n",
    "    \"A mutual fund is an investment vehicle that pools money from many investors to purchase securities.\",\n",
    "    \"The Consumer Price Index (CPI) is a measure that examines the weighted average of prices of a basket of consumer goods and services.\",\n",
    "    \"The inflation rate refers to the rate at which the general level of prices for goods and services rises and erodes purchasing power.\",\n",
    "    \"Financial diversification involves spreading investments across various asset classes to reduce risk.\",\n",
    "    \"A stock dividend is a payment made by a corporation to its shareholders, usually in the form of additional shares or cash.\",\n",
    "    \"Capital gains are the profits earned from the sale of an asset, such as a stock, bond, or real estate property.\",\n",
    "    \"Tax planning is the process of analyzing financial situations to minimize tax liability through various strategies.\",\n",
    "    \"The money market is a sector of the financial market in which short-term borrowing and lending takes place.\",\n",
    "    \"A mortgage is a loan specifically used to purchase real estate, typically involving regular payments of principal and interest.\",\n",
    "    \"A retirement plan is a financial arrangement designed to provide income during retirement years.\",\n",
    "    \"Asset allocation is the strategy of distributing investments across various asset classes, such as stocks, bonds, and real estate.\",\n",
    "    \"A financial advisor is a professional who helps clients manage their investments, estate planning, and financial goals.\",\n",
    "    \"Corporate finance involves managing a company's financial activities, such as investment decisions, capital raising, and risk management.\",\n",
    "    \"An annuity is a financial product that provides a series of payments made at equal intervals, often used for retirement income.\",\n",
    "    \"An emergency fund is a reserve of money set aside to cover unexpected expenses or financial emergencies.\",\n",
    "    \"A credit card allows users to borrow funds up to a limit to make purchases, with interest charged on outstanding balances.\",\n",
    "    \"A pension plan is a retirement plan where employers make contributions to a pool of funds set aside for an employee's future benefit.\",\n",
    "    \"Debt consolidation is the process of combining multiple debts into a single loan or payment plan to simplify management.\",\n",
    "    \"A financial statement is a formal record of a company's financial activities and position, including the balance sheet and income statement.\",\n",
    "    \"The balance sheet is a financial statement that reports a company's assets, liabilities, and shareholders' equity at a specific point in time.\",\n",
    "    \"An income statement is a financial document that shows a company's revenues and expenses over a specific period of time.\",\n",
    "    \"Financial modeling is the process of creating a mathematical representation of a company's financial performance.\",\n",
    "    \"Venture capital refers to funding provided to startups or small businesses with high growth potential in exchange for equity.\",\n",
    "    \"Private equity refers to investments made in privately held companies, typically through buyouts or direct investments.\",\n",
    "    \"A dividend yield is the annual dividend payment divided by the stock's price, representing the return an investor can expect from dividends.\",\n",
    "    \"Asset management involves managing investments on behalf of clients, often through mutual funds, ETFs, or other financial products.\",\n",
    "    \"Financial leverage involves using borrowed capital to increase the potential return on investment, though it also increases risk.\",\n",
    "    \"A liquidity ratio is a financial metric that measures a company's ability to meet its short-term obligations using its liquid assets.\",\n",
    "    \"Debt-to-equity ratio is a financial leverage ratio that compares a company's total liabilities to its shareholder equity.\",\n",
    "    \"A treasury bond is a debt security issued by the government with a fixed interest rate and a maturity of 10 years or more.\",\n",
    "    \"A government bond is a debt instrument issued by a national government to support spending and obligations.\",\n",
    "    \"An investment portfolio is a collection of assets held by an individual or institution for the purpose of achieving specific financial goals.\",\n",
    "    \"Foreign exchange (Forex) is the global marketplace for trading currencies, driven by factors like interest rates and economic stability.\",\n",
    "    \"Financial independence is the state of having sufficient income or wealth to cover all living expenses without needing employment.\",\n",
    "    \"Personal finance refers to the management of an individual's or family's financial activities, such as budgeting, investing, and saving.\",\n",
    "    \"A wealth manager is a financial advisor who provides specialized services in managing high-net-worth individuals' assets and investments.\",\n",
    "    \"Crowdfunding involves raising small amounts of money from a large number of people, typically via the internet, to fund a project or venture.\",\n",
    "    \"A commodity is a basic good used in commerce that is interchangeable with other goods of the same type, such as oil or gold.\",\n",
    "    \"A real estate investment trust (REIT) is a company that owns, operates, or finances income-producing real estate.\",\n",
    "    \"A cryptocurrency wallet is a digital tool used to store and manage cryptocurrency assets like Bitcoin or Ethereum.\",\n",
    "    \"A decentralized finance (DeFi) platform is a blockchain-based financial service that operates without a centralized authority.\",\n",
    "    \"The term 'blockchain' refers to a distributed ledger technology used to securely store data in a decentralized manner.\",\n",
    "    \"A smart contract is a self-executing contract with terms directly written into code that automatically enforces the contract's terms.\",\n",
    "    \"Financial risk management involves identifying, analyzing, and mitigating risks to minimize the financial impact of uncertain events.\",\n",
    "    \"A stock buyback occurs when a company repurchases its own shares from the market, reducing the number of outstanding shares.\",\n",
    "    \"Market capitalization refers to the total value of a company's outstanding shares of stock, calculated by multiplying share price by shares outstanding.\",\n",
    "    \"A short sale occurs when an investor borrows shares to sell them at a high price, hoping to buy them back later at a lower price.\",\n",
    "    \"A portfolio manager is a professional responsible for making investment decisions and managing an investment portfolio on behalf of clients.\",\n",
    "    \"Financial leverage can increase potential returns but also amplifies risk, especially if investments do not perform as expected.\",\n",
    "    \"A margin account allows an investor to borrow funds from a broker to purchase securities, increasing buying power.\",\n",
    "    \"A credit default swap is a financial derivative contract that allows investors to swap the credit risk of bond issues.\",\n",
    "    \"The term 'bear market' refers to a market condition in which asset prices are falling or expected to fall.\",\n",
    "    \"The term 'bull market' refers to a market condition where asset prices are rising or expected to rise.\",\n",
    "    \"The price-to-earnings (P/E) ratio is a valuation ratio calculated by dividing a company's share price by its earnings per share (EPS).\",\n",
    "    \"A blue-chip stock refers to shares of a well-established company with a history of stable performance and reliability.\",\n",
    "    \"The bond yield curve is a graph that shows the relationship between bond yields and maturities for bonds of similar credit quality.\",\n",
    "    \"An economic recession is a significant decline in economic activity spread across the economy, lasting for several months or more.\",\n",
    "    \"Interest rate hikes typically lead to reduced borrowing and spending, which can slow down economic growth and lower inflation.\",\n",
    "    \"A bank run occurs when a large number of depositors attempt to withdraw their funds simultaneously, fearing the bank's insolvency.\",\n",
    "    \"Behavioral finance studies the psychological influences on investor decisions and how they affect market outcomes.\",\n",
    "    \"A financial crisis is a situation in which the value of financial assets or institutions drops rapidly, potentially leading to systemic instability.\",\n",
    "    \"Sustainable investing focuses on investments that generate social and environmental benefits alongside financial returns.\",\n",
    "    \"Impact investing seeks to generate a positive social or environmental impact alongside financial returns.\"\n",
    "    ]\n",
    "    index = load_faiss_index(embedding_model, corpus)\n",
    "\n",
    "    new_documents = [\n",
    "        \"Machine learning is a subset of artificial intelligence that enables systems to improve from experience.\",\n",
    "        \"Bitcoin is a decentralized digital currency that operates without a central authority or government.\"\n",
    "    ]\n",
    "    update_faiss_index(index, new_documents, embedding_model)\n",
    "\n",
    "    documents = retrieve_documents(query, index, embedding_model)\n",
    "\n",
    "    context = \"\\n\".join(documents)\n",
    "\n",
    "    generated_answer = generate_answer(qa_pipeline, query, context)\n",
    "    print(f\"Generated Answer: {generated_answer}\")\n",
    "\n",
    "    rouge_scores = evaluate_rouge(reference_answer, generated_answer)\n",
    "    print(f\"ROUGE Scores: {rouge_scores}\")\n",
    "\n",
    "def evaluate_rouge(reference, generated):\n",
    "    from rouge_score import rouge_scorer\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    scores = scorer.score(reference, generated)\n",
    "    return scores\n",
    "\n",
    "# Exemple d'utilisation\n",
    "query = \"What is the stock market?\"\n",
    "reference_answer = \"The stock market is a place where buyers and sellers trade shares of public companies.\"\n",
    "test_rag_system(query, reference_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jkHcFpdTOir"
   },
   "source": [
    "### ROUGE Scores Analysis\n",
    "\n",
    "#### ROUGE-1\n",
    "- **Precision**: <span style=\"color: green;\">0.7857</span>  \n",
    "  Mesure combien des unigrams (mots individuels) générés par le modèle apparaissent dans la référence.\n",
    "- **Recall**: <span style=\"color: orange;\">0.7333</span>  \n",
    " Mesure combien des unigrams de la référence se trouvent dans le texte généré.\n",
    "- **F-Measure**: <span style=\"color: blue;\">0.7586</span>  \n",
    "  Une moyenne harmonique de la précision et du rappel, offrant une vue équilibrée des performances du modèle.\n",
    "\n",
    "**Interpretation**:  \n",
    "\n",
    "Le modèle performe bien dans la correspondance des unigrams, avec une précision de **78,57%** et un rappel de **73,33%.** La  F measure de **75,86%** suggère une forte performance globale dans la correspondance des unigrams.\n",
    "\n",
    "---\n",
    "\n",
    "#### ROUGE-2\n",
    "- **Precision**: <span style=\"color: green;\">0.6923</span>  \n",
    "  Measures how many of the bigrams (pairs of consecutive words) generated by the model match the reference.\n",
    "- **Recall**: <span style=\"color: orange;\">0.6429</span>  \n",
    "  Measures how many of the bigrams from the reference are found in the generated text.\n",
    "- **F-Measure**: <span style=\"color: blue;\">0.6667</span>  \n",
    "  Une mesure équilibrée de la précision et du rappel pour les bigrammes.\n",
    "**Interpretation**:  \n",
    "Le modèle performe légèrement moins bien avec les bigrammes, obtenant une précision de **69,23%** et un rappel de **64,29%**. La mesure F de **66,67%** suggère qu'il y a encore de la marge pour améliorer la capture des relations entre les bigrammes.\n",
    "\n",
    "---\n",
    "\n",
    "#### ROUGE-L\n",
    "- **Precision**: <span style=\"color: green;\">0.7857</span>  \n",
    "- **Recall**: <span style=\"color: orange;\">0.7333</span>  \n",
    "- **F-Measure**: <span style=\"color: blue;\">0.7586</span>  \n",
    "\n",
    "**Interpretation**:  \n",
    "ROUGE-L évalue la plus longue sous-séquence commune (LCS), en tenant compte de l'ordre des mots. Les scores sont identiques à ceux de ROUGE-1, ce qui indique que le texte généré s'aligne bien avec la référence en termes de plus longues sous-séquences.\n",
    "---\n",
    "\n",
    "### **Summary Interpretation**:\n",
    "- Les **hauts scores** ROUGE-1 et ROUGE-L suggèrent que le modèle performe bien dans la correspondance globale des mots et des sous-séquences.\n",
    "  \n",
    "- Dans l'ensemble, le modèle est pertinent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPArr_M_UrJr"
   },
   "source": [
    "## Comparaison finale :\n",
    " les meilleurs résultats sont données par RAG suivi du peft suivi du prompt engineering few shot"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (Pyodide)",
   "language": "python",
   "name": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "049f6e54c97f4a7bb6269f51ca3663fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1d351cf4da9442aeaeb3aa3e297128b7",
       "IPY_MODEL_dd34c17ea0f14a3c8f81bccaf3baa22a",
       "IPY_MODEL_5e77643866d1468a81f6bf8efaa25ed1"
      ],
      "layout": "IPY_MODEL_880c698bc4a64e5cb177e83836049f5c"
     }
    },
    "0c58a284f0564ad5bc9b8765c80d383f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1bd9a5486b2e48189d6fe208967b1abb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d351cf4da9442aeaeb3aa3e297128b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_310b257905154887b4cb49363923b52b",
      "placeholder": "​",
      "style": "IPY_MODEL_ce71d8ea497140daab88316c88704061",
      "value": "Map: 100%"
     }
    },
    "2a0c49020dec4680bcf6205c4ffc6294": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b2799e24d9a4e2e835a13693d212cc6",
      "placeholder": "​",
      "style": "IPY_MODEL_5c957206c55740dc8d4f987cf75d04a6",
      "value": "0.023 MB of 0.023 MB uploaded\r"
     }
    },
    "310b257905154887b4cb49363923b52b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3bff57bcdff1434a952716c0e4a19894": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c957206c55740dc8d4f987cf75d04a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e77643866d1468a81f6bf8efaa25ed1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3bff57bcdff1434a952716c0e4a19894",
      "placeholder": "​",
      "style": "IPY_MODEL_aadd10fe0aa846cda261db0baae797e4",
      "value": " 1000/1000 [00:00&lt;00:00, 2582.17 examples/s]"
     }
    },
    "62560eee08ba4afab1375d0146dad46b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6b2799e24d9a4e2e835a13693d212cc6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "749453296f1846ba858e063d67655b69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2a0c49020dec4680bcf6205c4ffc6294",
       "IPY_MODEL_d80fd6a5efea44e5a6e9c5ff10e544a9"
      ],
      "layout": "IPY_MODEL_a19df65cf482477ab95aa8201e41047c"
     }
    },
    "880c698bc4a64e5cb177e83836049f5c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a19df65cf482477ab95aa8201e41047c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aadd10fe0aa846cda261db0baae797e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cda7c97514b144cbb3a78ee9c82de964": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ce71d8ea497140daab88316c88704061": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d80fd6a5efea44e5a6e9c5ff10e544a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c58a284f0564ad5bc9b8765c80d383f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_62560eee08ba4afab1375d0146dad46b",
      "value": 1
     }
    },
    "dd34c17ea0f14a3c8f81bccaf3baa22a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1bd9a5486b2e48189d6fe208967b1abb",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cda7c97514b144cbb3a78ee9c82de964",
      "value": 1000
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
